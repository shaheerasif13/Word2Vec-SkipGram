{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55612c7f",
   "metadata": {},
   "source": [
    "##### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f903a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc9a57",
   "metadata": {},
   "source": [
    "##### Custom class of Word2Vec Skip-gram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2ef4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Word2Vec Skip-gram model\n",
    "class Word2VecSkipGram():\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, sentences, vector_size, window, learning_rate, epochs, verbose=False):\n",
    "        self.h = None\n",
    "        self.u = None\n",
    "        self.y = None\n",
    "        self.loss = 0\n",
    "        self.vector_size = vector_size # Number of neurons\n",
    "        self.window = window\n",
    "        self.epochs = epochs;\n",
    "        self.verbose = verbose\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training_samples, self.vocabulary, self.vocabulary_size = self.create_vectors(sentences)\n",
    "        self.word_vector_weights = np.random.uniform(-0.9, 0.9, (self.vocabulary_size, self.vector_size))\n",
    "        self.context_vector_weights = np.random.uniform(-0.9, 0.9, (self.vector_size, self.vocabulary_size))\n",
    "        self.train()\n",
    "        \n",
    "    # Method to create vectors (One-hot encoded embeddings) of words and their context words\n",
    "    def create_vectors(self, sentences):\n",
    "        \n",
    "        # Creating a vocabulary (Dictionary with word and its index)\n",
    "        all_words = []\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                all_words.append(word)\n",
    "        unique_words = sorted(list(set(all_words)))\n",
    "        vocabulary = {}\n",
    "        for i in range(len(unique_words)):\n",
    "            vocabulary[unique_words[i]] = i\n",
    "        vocabulary_size = len(vocabulary)\n",
    "\n",
    "        # Creating vectors (Training samples) for all words and their context words\n",
    "        training_samples = []\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence)):\n",
    "\n",
    "                # Creating a target word vector\n",
    "                word_vector = [0 for _ in range(vocabulary_size)]\n",
    "                word_vector[vocabulary[sentence[i]]] = 1\n",
    "\n",
    "                # Creating context words vector\n",
    "                context_vector = [0 for _ in range(vocabulary_size)]\n",
    "                for j in range(i - self.window, i + self.window):\n",
    "                    if j != i and j >= 0 and j < len(sentence):\n",
    "                        context_vector[vocabulary[sentence[j]]] = 1\n",
    "                        \n",
    "                # Creating training sample tuple for current word (word_vector, context_vector)\n",
    "                training_samples.append((word_vector, context_vector))                                \n",
    "        \n",
    "        return training_samples, vocabulary, vocabulary_size\n",
    "        \n",
    "    # Softmax activation function\n",
    "    def softmax(self, vector):\n",
    "        e_x = np.exp(vector - np.max(vector))\n",
    "        return e_x / e_x.sum()\n",
    "        \n",
    "    # Method to perform forward pass\n",
    "    def forward_pass(self, word_vector):\n",
    "        self.h = np.dot(self.word_vector_weights.T, word_vector).reshape(self.vector_size, 1)\n",
    "        self.u = np.dot(self.context_vector_weights.T, self.h)  \n",
    "        self.y = self.softmax(self.u)\n",
    "        return self.y\n",
    "    \n",
    "    # Method to perform back propagation\n",
    "    def back_propagation(self, word_vector, context_vector):\n",
    "        \n",
    "        # Computing error for context words\n",
    "        e = self.y - np.asarray(context_vector).reshape(self.vocabulary_size, 1)\n",
    "        \n",
    "        # Calculating new weights\n",
    "        new_context_vector_weights = np.dot(self.h, e.T)\n",
    "        new_word_vector_weights = np.dot(np.array(word_vector).reshape(self.vocabulary_size, 1), np.dot(self.context_vector_weights, e).T)\n",
    "        \n",
    "        # Changing weights using back propagation\n",
    "        self.context_vector_weights -= self.learning_rate * new_context_vector_weights\n",
    "        self.word_vector_weights -= self.learning_rate * new_word_vector_weights\n",
    "        \n",
    "    # Method to calculate loss\n",
    "    def calculate_loss(self, context_vector):\n",
    "        C = 0\n",
    "        for i in range(self.vocabulary_size):\n",
    "            if context_vector[i]:\n",
    "                self.loss += -1 * self.u[i][0]\n",
    "                C += 1\n",
    "        self.loss += C * np.log(np.sum(np.exp(self.u)))\n",
    "        \n",
    "    # Method to perform training of model\n",
    "    def train(self):\n",
    "        for i in range(0, self.epochs):\n",
    "            self.loss = 0\n",
    "            for word_vector, context_vector in self.training_samples:\n",
    "                \n",
    "                # Performing forward pass\n",
    "                self.forward_pass(word_vector)\n",
    "                \n",
    "                # Performing back propagation\n",
    "                self.back_propagation(word_vector, context_vector)\n",
    "                \n",
    "                # Calculating loss\n",
    "                self.calculate_loss(context_vector)\n",
    "                \n",
    "            # Adaptive learning rate\n",
    "            self.learning_rate *= 1 / (1 + self.learning_rate * i)\n",
    "            \n",
    "            # Displaying loss after every epoch\n",
    "            if self.verbose:\n",
    "                print(\"Epoch: \", i, \", Loss: \", self.loss, sep=\"\")\n",
    "                \n",
    "    # Method to get most similar words for a given word\n",
    "    def most_similar(self, word, top=10):\n",
    "        if word in list(self.vocabulary.keys()):\n",
    "            \n",
    "            # Create word vector of given word\n",
    "            word_vector = [0 if i != self.vocabulary[word] else 1 for i in range(self.vocabulary_size)]\n",
    "            \n",
    "            # Performing forward pass\n",
    "            prediction = self.forward_pass(word_vector)\n",
    "            \n",
    "            # Creating a dictionary of all words and their scores\n",
    "            vocabulary = list(self.vocabulary)\n",
    "            word_scores = {}\n",
    "            for i in range(len(vocabulary)):\n",
    "                if vocabulary[i] != word:\n",
    "                    word_scores[vocabulary[i]] = prediction[i][0]\n",
    "                \n",
    "            # Sorting dictionary of all words and their scores\n",
    "            word_scores = sorted(word_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "            \n",
    "            return word_scores[:top]\n",
    "        return None\n",
    "    \n",
    "    # Method to get weight vector (Embedding) of given word\n",
    "    def get_embedding(self, word):\n",
    "        if word in list(self.vocabulary.keys()):\n",
    "            return self.word_vector_weights[self.vocabulary[word]]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d67167f",
   "metadata": {},
   "source": [
    "##### Utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "491ee257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(word_embedding1, word_embedding2):\n",
    "    a = word_embedding1\n",
    "    b= word_embedding2\n",
    "    return np.dot(word_embedding1, word_embedding2) / np.sqrt(np.dot(word_embedding1, word_embedding1) * np.dot(word_embedding2, word_embedding2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d5570",
   "metadata": {},
   "source": [
    "##### Loading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd8100a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today we will be learning about the fundamentals of data science and\\nstatistics. Data Science and statistics are hot and growing fields with\\nalternative names of machine learning, artificial intelligence, big\\ndata, etc. I'm really excited to talk to you about data science and\\nstatistics because data science and statistics have long been passions\\nof mine. I didn't use to be very good at data science and statistics but\\nafter studying data science and statistics for a long time, I got\\nbetter and better at it until I became a data science and statistics\\nexpert. I'm really excited to talk to you about data science and\\nstatistics, thanks for listening to me talk about data science and\\nstatistics.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data from file\n",
    "with open(\"data.txt\", \"r\") as in_file:\n",
    "    data = in_file.read()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8611db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today we will be learning about the fundamentals of data science and\\nstatistics.',\n",
       " 'Data Science and statistics are hot and growing fields with\\nalternative names of machine learning, artificial intelligence, big\\ndata, etc.',\n",
       " \"I'm really excited to talk to you about data science and\\nstatistics because data science and statistics have long been passions\\nof mine.\",\n",
       " \"I didn't use to be very good at data science and statistics but\\nafter studying data science and statistics for a long time, I got\\nbetter and better at it until I became a data science and statistics\\nexpert.\",\n",
       " \"I'm really excited to talk to you about data science and\\nstatistics, thanks for listening to me talk about data science and\\nstatistics.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(data)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d424c",
   "metadata": {},
   "source": [
    "##### Data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1244c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today we will be learning about the fundamentals of data science and statistics.',\n",
       " 'Data Science and statistics are hot and growing fields with alternative names of machine learning, artificial intelligence, big data, etc.',\n",
       " \"I'm really excited to talk to you about data science and statistics because data science and statistics have long been passions of mine.\",\n",
       " \"I didn't use to be very good at data science and statistics but after studying data science and statistics for a long time, I got better and better at it until I became a data science and statistics expert.\",\n",
       " \"I'm really excited to talk to you about data science and statistics, thanks for listening to me talk about data science and statistics.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing new line characters\n",
    "sentences = [sentence.replace(\"\\n\", \" \") for sentence in sentences]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a3a5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today we will be learning about the fundamentals of data science and statistics.',\n",
       " 'data science and statistics are hot and growing fields with alternative names of machine learning, artificial intelligence, big data, etc.',\n",
       " \"i'm really excited to talk to you about data science and statistics because data science and statistics have long been passions of mine.\",\n",
       " \"i didn't use to be very good at data science and statistics but after studying data science and statistics for a long time, i got better and better at it until i became a data science and statistics expert.\",\n",
       " \"i'm really excited to talk to you about data science and statistics, thanks for listening to me talk about data science and statistics.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making dataset lowercase\n",
    "sentences = [sentence.casefold() for sentence in sentences]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9336f992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today we will be learning about the fundamentals of data science and statistics',\n",
       " 'data science and statistics are hot and growing fields with alternative names of machine learning artificial intelligence big data etc',\n",
       " 'im really excited to talk to you about data science and statistics because data science and statistics have long been passions of mine',\n",
       " 'i didnt use to be very good at data science and statistics but after studying data science and statistics for a long time i got better and better at it until i became a data science and statistics expert',\n",
       " 'im really excited to talk to you about data science and statistics thanks for listening to me talk about data science and statistics']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing punctuation from dataset\n",
    "sentences = [sentence.translate(sentence.maketrans(\"\", \"\", string.punctuation)) for sentence in sentences]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3092120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['today', 'we', 'will', 'be', 'learning', 'about', 'the', 'fundamentals', 'of', 'data', 'science', 'and', 'statistics'], ['data', 'science', 'and', 'statistics', 'are', 'hot', 'and', 'growing', 'fields', 'with', 'alternative', 'names', 'of', 'machine', 'learning', 'artificial', 'intelligence', 'big', 'data', 'etc'], ['im', 'really', 'excited', 'to', 'talk', 'to', 'you', 'about', 'data', 'science', 'and', 'statistics', 'because', 'data', 'science', 'and', 'statistics', 'have', 'long', 'been', 'passions', 'of', 'mine'], ['i', 'didnt', 'use', 'to', 'be', 'very', 'good', 'at', 'data', 'science', 'and', 'statistics', 'but', 'after', 'studying', 'data', 'science', 'and', 'statistics', 'for', 'a', 'long', 'time', 'i', 'got', 'better', 'and', 'better', 'at', 'it', 'until', 'i', 'became', 'a', 'data', 'science', 'and', 'statistics', 'expert'], ['im', 'really', 'excited', 'to', 'talk', 'to', 'you', 'about', 'data', 'science', 'and', 'statistics', 'thanks', 'for', 'listening', 'to', 'me', 'talk', 'about', 'data', 'science', 'and', 'statistics']]\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e846029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['today', 'learning', 'fundamentals', 'data', 'science', 'statistics'], ['data', 'science', 'statistics', 'hot', 'growing', 'fields', 'alternative', 'names', 'machine', 'learning', 'artificial', 'intelligence', 'big', 'data', 'etc'], ['im', 'really', 'excited', 'talk', 'data', 'science', 'statistics', 'data', 'science', 'statistics', 'long', 'passions', 'mine'], ['didnt', 'use', 'good', 'data', 'science', 'statistics', 'studying', 'data', 'science', 'statistics', 'long', 'time', 'got', 'better', 'better', 'became', 'data', 'science', 'statistics', 'expert'], ['im', 'really', 'excited', 'talk', 'data', 'science', 'statistics', 'thanks', 'listening', 'talk', 'data', 'science', 'statistics']]\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words from dataset\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_sentences = []\n",
    "for sentence in sentences:\n",
    "    original_sentence = sentence\n",
    "    sentence = [word for word in sentence if word not in stop_words]\n",
    "    if len(sentence) < 1:\n",
    "        sentence = original_sentence\n",
    "    filtered_sentences.append(sentence)\n",
    "sentences = filtered_sentences\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d75a1b",
   "metadata": {},
   "source": [
    "#####  Word2Vec (Skip-gram) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "018915ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1010.4370621166835\n",
      "Epoch: 1, Loss: 1008.286403227592\n",
      "Epoch: 2, Loss: 1006.1531127118399\n",
      "Epoch: 3, Loss: 1004.036855780981\n",
      "Epoch: 4, Loss: 1001.9373392554968\n",
      "Epoch: 5, Loss: 999.8543198130064\n",
      "Epoch: 6, Loss: 997.7876112745554\n",
      "Epoch: 7, Loss: 995.7370907910096\n",
      "Epoch: 8, Loss: 993.7027038193805\n",
      "Epoch: 9, Loss: 991.6844678093704\n",
      "Epoch: 10, Loss: 989.6824745524144\n",
      "Epoch: 11, Loss: 987.6968911778624\n",
      "Epoch: 12, Loss: 985.727959812491\n",
      "Epoch: 13, Loss: 983.7759959492895\n",
      "Epoch: 14, Loss: 981.8413855984904\n",
      "Epoch: 15, Loss: 979.9245813173684\n",
      "Epoch: 16, Loss: 978.0260972349549\n",
      "Epoch: 17, Loss: 976.1465032029964\n",
      "Epoch: 18, Loss: 974.2864182153612\n",
      "Epoch: 19, Loss: 972.4465032444394\n",
      "Epoch: 20, Loss: 970.6274536452374\n",
      "Epoch: 21, Loss: 968.8299912762287\n",
      "Epoch: 22, Loss: 967.0548564808503\n",
      "Epoch: 23, Loss: 965.3028000655272\n",
      "Epoch: 24, Loss: 963.574575399683\n",
      "Epoch: 25, Loss: 961.8709307509355\n",
      "Epoch: 26, Loss: 960.192601955216\n",
      "Epoch: 27, Loss: 958.540305507198\n",
      "Epoch: 28, Loss: 956.9147321418654\n",
      "Epoch: 29, Loss: 955.3165409635219\n",
      "Epoch: 30, Loss: 953.7463541644379\n",
      "Epoch: 31, Loss: 952.2047523619883\n",
      "Epoch: 32, Loss: 950.6922705706385\n",
      "Epoch: 33, Loss: 949.2093948137882\n",
      "Epoch: 34, Loss: 947.7565593702454\n",
      "Epoch: 35, Loss: 946.3341446412035\n",
      "Epoch: 36, Loss: 944.9424756158324\n",
      "Epoch: 37, Loss: 943.5818209072362\n",
      "Epoch: 38, Loss: 942.252392325204\n",
      "Epoch: 39, Loss: 940.9543449481737\n",
      "Epoch: 40, Loss: 939.6877776537594\n",
      "Epoch: 41, Loss: 938.4527340651914\n",
      "Epoch: 42, Loss: 937.2492038698815\n",
      "Epoch: 43, Loss: 936.0771244659654\n",
      "Epoch: 44, Loss: 934.9363828930575\n",
      "Epoch: 45, Loss: 933.8268180043762\n",
      "Epoch: 46, Loss: 932.7482228389006\n",
      "Epoch: 47, Loss: 931.7003471540472\n",
      "Epoch: 48, Loss: 930.6829000816143\n",
      "Epoch: 49, Loss: 929.6955528721778\n",
      "Epoch: 50, Loss: 928.7379416957602\n",
      "Epoch: 51, Loss: 927.8096704694112\n",
      "Epoch: 52, Loss: 926.9103136850805\n",
      "Epoch: 53, Loss: 926.0394192140805\n",
      "Epoch: 54, Loss: 925.1965110671888\n",
      "Epoch: 55, Loss: 924.3810920921526\n",
      "Epoch: 56, Loss: 923.5926465930556\n",
      "Epoch: 57, Loss: 922.8306428584087\n",
      "Epoch: 58, Loss: 922.0945355872498\n",
      "Epoch: 59, Loss: 921.3837682047084\n",
      "Epoch: 60, Loss: 920.697775060478\n",
      "Epoch: 61, Loss: 920.0359835055466\n",
      "Epoch: 62, Loss: 919.3978158441427\n",
      "Epoch: 63, Loss: 918.7826911593816\n",
      "Epoch: 64, Loss: 918.1900270124012\n",
      "Epoch: 65, Loss: 917.6192410159416\n",
      "Epoch: 66, Loss: 917.0697522842944\n",
      "Epoch: 67, Loss: 916.540982762438\n",
      "Epoch: 68, Loss: 916.0323584378143\n",
      "Epoch: 69, Loss: 915.5433104388582\n",
      "Epoch: 70, Loss: 915.0732760247859\n",
      "Epoch: 71, Loss: 914.6216994715306\n",
      "Epoch: 72, Loss: 914.1880328589859\n",
      "Epoch: 73, Loss: 913.7717367648503\n",
      "Epoch: 74, Loss: 913.3722808704958\n",
      "Epoch: 75, Loss: 912.9891444843114\n",
      "Epoch: 76, Loss: 912.621816987924\n",
      "Epoch: 77, Loss: 912.2697982106622\n",
      "Epoch: 78, Loss: 911.9325987374916\n",
      "Epoch: 79, Loss: 911.6097401555045\n",
      "Epoch: 80, Loss: 911.3007552439057\n",
      "Epoch: 81, Loss: 911.0051881121954\n",
      "Epoch: 82, Loss: 910.7225942910857\n",
      "Epoch: 83, Loss: 910.4525407804499\n",
      "Epoch: 84, Loss: 910.1946060583755\n",
      "Epoch: 85, Loss: 909.9483800551582\n",
      "Epoch: 86, Loss: 909.7134640958838\n",
      "Epoch: 87, Loss: 909.4894708149335\n",
      "Epoch: 88, Loss: 909.2760240456174\n",
      "Epoch: 89, Loss: 909.072758687852\n",
      "Epoch: 90, Loss: 908.879320556617\n",
      "Epoch: 91, Loss: 908.6953662137036\n",
      "Epoch: 92, Loss: 908.5205627850881\n",
      "Epoch: 93, Loss: 908.3545877660574\n",
      "Epoch: 94, Loss: 908.1971288160446\n",
      "Epoch: 95, Loss: 908.0478835449601\n",
      "Epoch: 96, Loss: 907.9065592926394\n",
      "Epoch: 97, Loss: 907.7728729028944\n",
      "Epoch: 98, Loss: 907.6465504934957\n",
      "Epoch: 99, Loss: 907.5273272232948\n"
     ]
    }
   ],
   "source": [
    "vector_size = 5\n",
    "window = 3\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "model = Word2VecSkipGram(sentences, vector_size, window, learning_rate, epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83bf8113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('statistics', 0.14859777221789916),\n",
       " ('science', 0.07893528809438585),\n",
       " ('good', 0.05704925368399773),\n",
       " ('became', 0.05512691988759127),\n",
       " ('talk', 0.047754659246576965),\n",
       " ('better', 0.03313228113539583),\n",
       " ('really', 0.03123615350212278),\n",
       " ('studying', 0.030067961617975696),\n",
       " ('big', 0.029765136960201312),\n",
       " ('fields', 0.0286821829338157)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd9a8f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 0.41371046381530585),\n",
       " ('science', 0.1336779523410498),\n",
       " ('became', 0.08664259790088952),\n",
       " ('good', 0.04534134002253001),\n",
       " ('talk', 0.0438375350344247),\n",
       " ('long', 0.03729519228312033),\n",
       " ('really', 0.02800214931207578),\n",
       " ('fields', 0.02475041681919624),\n",
       " ('better', 0.014835301207424666),\n",
       " ('expert', 0.013819005960043934)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de313b4a",
   "metadata": {},
   "source": [
    "##### Computing cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a529658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternative</th>\n",
       "      <th>artificial</th>\n",
       "      <th>became</th>\n",
       "      <th>better</th>\n",
       "      <th>big</th>\n",
       "      <th>data</th>\n",
       "      <th>didnt</th>\n",
       "      <th>etc</th>\n",
       "      <th>excited</th>\n",
       "      <th>expert</th>\n",
       "      <th>...</th>\n",
       "      <th>passions</th>\n",
       "      <th>really</th>\n",
       "      <th>science</th>\n",
       "      <th>statistics</th>\n",
       "      <th>studying</th>\n",
       "      <th>talk</th>\n",
       "      <th>thanks</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alternative</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.285043</td>\n",
       "      <td>0.053769</td>\n",
       "      <td>-0.679491</td>\n",
       "      <td>0.096176</td>\n",
       "      <td>0.446685</td>\n",
       "      <td>-0.017968</td>\n",
       "      <td>-0.210450</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>-0.280719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197267</td>\n",
       "      <td>-0.434694</td>\n",
       "      <td>-0.167805</td>\n",
       "      <td>0.203478</td>\n",
       "      <td>0.356667</td>\n",
       "      <td>0.745365</td>\n",
       "      <td>-0.029041</td>\n",
       "      <td>0.365627</td>\n",
       "      <td>0.805696</td>\n",
       "      <td>-0.298269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artificial</th>\n",
       "      <td>-0.285043</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.104594</td>\n",
       "      <td>0.865117</td>\n",
       "      <td>-0.291089</td>\n",
       "      <td>0.269344</td>\n",
       "      <td>-0.321764</td>\n",
       "      <td>-0.487023</td>\n",
       "      <td>0.833241</td>\n",
       "      <td>-0.654566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417090</td>\n",
       "      <td>-0.524127</td>\n",
       "      <td>0.333866</td>\n",
       "      <td>-0.204251</td>\n",
       "      <td>-0.063536</td>\n",
       "      <td>-0.206273</td>\n",
       "      <td>-0.385485</td>\n",
       "      <td>-0.432621</td>\n",
       "      <td>-0.113201</td>\n",
       "      <td>0.382615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>became</th>\n",
       "      <td>0.053769</td>\n",
       "      <td>-0.104594</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.173582</td>\n",
       "      <td>-0.674288</td>\n",
       "      <td>-0.505346</td>\n",
       "      <td>0.674966</td>\n",
       "      <td>-0.197224</td>\n",
       "      <td>-0.378414</td>\n",
       "      <td>-0.104401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055766</td>\n",
       "      <td>-0.497719</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.229863</td>\n",
       "      <td>-0.745073</td>\n",
       "      <td>-0.247090</td>\n",
       "      <td>0.459940</td>\n",
       "      <td>-0.015184</td>\n",
       "      <td>0.329703</td>\n",
       "      <td>-0.923156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better</th>\n",
       "      <td>-0.679491</td>\n",
       "      <td>0.865117</td>\n",
       "      <td>-0.173582</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.338618</td>\n",
       "      <td>0.144273</td>\n",
       "      <td>-0.133794</td>\n",
       "      <td>-0.267717</td>\n",
       "      <td>0.676114</td>\n",
       "      <td>-0.304351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211820</td>\n",
       "      <td>-0.189377</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>-0.063513</td>\n",
       "      <td>-0.057351</td>\n",
       "      <td>-0.359567</td>\n",
       "      <td>-0.146419</td>\n",
       "      <td>-0.324189</td>\n",
       "      <td>-0.384291</td>\n",
       "      <td>0.495798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>0.096176</td>\n",
       "      <td>-0.291089</td>\n",
       "      <td>-0.674288</td>\n",
       "      <td>-0.338618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.109615</td>\n",
       "      <td>-0.799045</td>\n",
       "      <td>0.359512</td>\n",
       "      <td>-0.184043</td>\n",
       "      <td>0.179161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348911</td>\n",
       "      <td>0.685902</td>\n",
       "      <td>-0.130032</td>\n",
       "      <td>-0.287401</td>\n",
       "      <td>0.258845</td>\n",
       "      <td>-0.098238</td>\n",
       "      <td>-0.650114</td>\n",
       "      <td>-0.300283</td>\n",
       "      <td>-0.483541</td>\n",
       "      <td>0.484026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.446685</td>\n",
       "      <td>0.269344</td>\n",
       "      <td>-0.505346</td>\n",
       "      <td>0.144273</td>\n",
       "      <td>-0.109615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.076411</td>\n",
       "      <td>-0.158209</td>\n",
       "      <td>0.697590</td>\n",
       "      <td>-0.178943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454560</td>\n",
       "      <td>-0.360867</td>\n",
       "      <td>0.765877</td>\n",
       "      <td>0.714018</td>\n",
       "      <td>0.810506</td>\n",
       "      <td>0.808092</td>\n",
       "      <td>-0.011499</td>\n",
       "      <td>0.445350</td>\n",
       "      <td>0.569706</td>\n",
       "      <td>0.433511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>didnt</th>\n",
       "      <td>-0.017968</td>\n",
       "      <td>-0.321764</td>\n",
       "      <td>0.674966</td>\n",
       "      <td>-0.133794</td>\n",
       "      <td>-0.799045</td>\n",
       "      <td>-0.076411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.092801</td>\n",
       "      <td>-0.358848</td>\n",
       "      <td>0.230520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>-0.274423</td>\n",
       "      <td>-0.057522</td>\n",
       "      <td>0.391909</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>0.204639</td>\n",
       "      <td>0.936601</td>\n",
       "      <td>0.613639</td>\n",
       "      <td>0.460470</td>\n",
       "      <td>-0.623865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etc</th>\n",
       "      <td>-0.210450</td>\n",
       "      <td>-0.487023</td>\n",
       "      <td>-0.197224</td>\n",
       "      <td>-0.267717</td>\n",
       "      <td>0.359512</td>\n",
       "      <td>-0.158209</td>\n",
       "      <td>-0.092801</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.212168</td>\n",
       "      <td>0.913285</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098404</td>\n",
       "      <td>0.604097</td>\n",
       "      <td>0.248404</td>\n",
       "      <td>0.362761</td>\n",
       "      <td>-0.132711</td>\n",
       "      <td>-0.256201</td>\n",
       "      <td>-0.200563</td>\n",
       "      <td>-0.254981</td>\n",
       "      <td>-0.296195</td>\n",
       "      <td>-0.041037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excited</th>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.833241</td>\n",
       "      <td>-0.378414</td>\n",
       "      <td>0.676114</td>\n",
       "      <td>-0.184043</td>\n",
       "      <td>0.697590</td>\n",
       "      <td>-0.358848</td>\n",
       "      <td>-0.212168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.411168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182676</td>\n",
       "      <td>-0.495829</td>\n",
       "      <td>0.706300</td>\n",
       "      <td>0.279263</td>\n",
       "      <td>0.306641</td>\n",
       "      <td>0.185451</td>\n",
       "      <td>-0.424756</td>\n",
       "      <td>-0.230805</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>0.489501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert</th>\n",
       "      <td>-0.280719</td>\n",
       "      <td>-0.654566</td>\n",
       "      <td>-0.104401</td>\n",
       "      <td>-0.304351</td>\n",
       "      <td>0.179161</td>\n",
       "      <td>-0.178943</td>\n",
       "      <td>0.230520</td>\n",
       "      <td>0.913285</td>\n",
       "      <td>-0.411168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249014</td>\n",
       "      <td>0.665488</td>\n",
       "      <td>0.239389</td>\n",
       "      <td>0.480184</td>\n",
       "      <td>-0.043200</td>\n",
       "      <td>-0.141606</td>\n",
       "      <td>0.191799</td>\n",
       "      <td>0.080122</td>\n",
       "      <td>-0.238450</td>\n",
       "      <td>-0.097380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             alternative  artificial    became    better       big      data  \\\n",
       "alternative     1.000000   -0.285043  0.053769 -0.679491  0.096176  0.446685   \n",
       "artificial     -0.285043    1.000000 -0.104594  0.865117 -0.291089  0.269344   \n",
       "became          0.053769   -0.104594  1.000000 -0.173582 -0.674288 -0.505346   \n",
       "better         -0.679491    0.865117 -0.173582  1.000000 -0.338618  0.144273   \n",
       "big             0.096176   -0.291089 -0.674288 -0.338618  1.000000 -0.109615   \n",
       "data            0.446685    0.269344 -0.505346  0.144273 -0.109615  1.000000   \n",
       "didnt          -0.017968   -0.321764  0.674966 -0.133794 -0.799045 -0.076411   \n",
       "etc            -0.210450   -0.487023 -0.197224 -0.267717  0.359512 -0.158209   \n",
       "excited        -0.000193    0.833241 -0.378414  0.676114 -0.184043  0.697590   \n",
       "expert         -0.280719   -0.654566 -0.104401 -0.304351  0.179161 -0.178943   \n",
       "\n",
       "                didnt       etc   excited    expert  ...  passions    really  \\\n",
       "alternative -0.017968 -0.210450 -0.000193 -0.280719  ...  0.197267 -0.434694   \n",
       "artificial  -0.321764 -0.487023  0.833241 -0.654566  ... -0.417090 -0.524127   \n",
       "became       0.674966 -0.197224 -0.378414 -0.104401  ... -0.055766 -0.497719   \n",
       "better      -0.133794 -0.267717  0.676114 -0.304351  ... -0.211820 -0.189377   \n",
       "big         -0.799045  0.359512 -0.184043  0.179161  ... -0.348911  0.685902   \n",
       "data        -0.076411 -0.158209  0.697590 -0.178943  ...  0.454560 -0.360867   \n",
       "didnt        1.000000 -0.092801 -0.358848  0.230520  ...  0.660714 -0.274423   \n",
       "etc         -0.092801  1.000000 -0.212168  0.913285  ... -0.098404  0.604097   \n",
       "excited     -0.358848 -0.212168  1.000000 -0.411168  ... -0.182676 -0.495829   \n",
       "expert       0.230520  0.913285 -0.411168  1.000000  ...  0.249014  0.665488   \n",
       "\n",
       "              science  statistics  studying      talk    thanks      time  \\\n",
       "alternative -0.167805    0.203478  0.356667  0.745365 -0.029041  0.365627   \n",
       "artificial   0.333866   -0.204251 -0.063536 -0.206273 -0.385485 -0.432621   \n",
       "became      -0.577099   -0.229863 -0.745073 -0.247090  0.459940 -0.015184   \n",
       "better       0.489451   -0.063513 -0.057351 -0.359567 -0.146419 -0.324189   \n",
       "big         -0.130032   -0.287401  0.258845 -0.098238 -0.650114 -0.300283   \n",
       "data         0.765877    0.714018  0.810506  0.808092 -0.011499  0.445350   \n",
       "didnt       -0.057522    0.391909 -0.168388  0.204639  0.936601  0.613639   \n",
       "etc          0.248404    0.362761 -0.132711 -0.256201 -0.200563 -0.254981   \n",
       "excited      0.706300    0.279263  0.306641  0.185451 -0.424756 -0.230805   \n",
       "expert       0.239389    0.480184 -0.043200 -0.141606  0.191799  0.080122   \n",
       "\n",
       "                today       use  \n",
       "alternative  0.805696 -0.298269  \n",
       "artificial  -0.113201  0.382615  \n",
       "became       0.329703 -0.923156  \n",
       "better      -0.384291  0.495798  \n",
       "big         -0.483541  0.484026  \n",
       "data         0.569706  0.433511  \n",
       "didnt        0.460470 -0.623865  \n",
       "etc         -0.296195 -0.041037  \n",
       "excited      0.158435  0.489501  \n",
       "expert      -0.238450 -0.097380  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix = np.zeros((model.vocabulary_size, model.vocabulary_size))\n",
    "for i in range(model.vocabulary_size):\n",
    "    for j in range(model.vocabulary_size):\n",
    "        cosine_similarity_matrix[i][j] = cosine_similarity(model.get_embedding(list(model.vocabulary)[i]), model.get_embedding(list(model.vocabulary)[j]))\n",
    "pd.DataFrame(data=cosine_similarity_matrix, columns=list(model.vocabulary), index=list(model.vocabulary)).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c89c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
